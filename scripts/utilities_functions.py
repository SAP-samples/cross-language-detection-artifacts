# scipy version 1.7.0 required 

import pandas as pd

import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, cross_val_score, cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from bayes_opt import BayesianOptimization




def split_training_testing(database,test_size,random):
    f = [c for c in database.columns if c not in ['Malicious','Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','Package Repository','Package Name']]#,'Number of lines in metadata','Number of Words in metadata'
    
    #regressor
    X  = database[f].iloc[:,:].values
   
    #target info 
    y  = database.loc[:,['Malicious','Package Repository','Package Name']].values
 
    
    #stratification based  benign/malicious and public repository origin ratio
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=True, stratify=y[:,0:2], random_state=random) # 0:2 stratify by ratio between malicious and benign and origin repo ratio 

    return(X_train, X_test, y_train, y_test,f)


'''
XGBOOST

'''

def evaluation_NPM_Pypi_xgb(database): 
    database = database.loc[:, ~database.columns.str.contains('^Unnamed')]
    database['Package Repository'] = np.where(database['Package Repository'] == "NPM", 1, 2)

    # Dict to set set of params and related precision
    hyperpar_list = []
    # define a list for the evaluation metrics 
    evaluation=['precision','recall','f1', 'accuracy','false positive','false negative','true negative','true positive','precision_npm','recall_npm','f1_npm', 'acc_npm','precision_pypi','recall_pypi','f1_pypi','acc_pypi']
    # define a list for the features 
    f = [c for c in database.columns if c not in ['Malicious','Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','Package Repository','Package Name']]#,'Number of lines in metadata','Number of Words in metadata'
    # initialize the dataframe
    eval = pd.DataFrame(data=None, index=[y for x in [f,evaluation] for y in x])
    random_split=[123,333,567,999,876,371,459,111,902,724]
    for i in range(0,len(random_split)):
        #split
        split_=split_training_testing(database[database['Package Repository']!=3], test_size=0.2, random=random_split[i])
        # optimization of the hyperparameters with cross-validation in the train set 
        train_rf_=grid_xgb_py(split_[0], split_[2])
        # fit the model with the best hyperparameters 
        classifier =xgb.XGBClassifier(random_state=123,n_estimators=train_rf_['n_estimators'],max_depth=train_rf_['max_depth'],gamma=train_rf_['gamma'],eta=train_rf_['eta'],colsample_bytree=train_rf_['colsample_bytree'],min_child_weight=train_rf_['min_child_weight'])
        classifier.fit(split_[0], split_[2][:,0].astype('int'))
        #predict on test data 
        y_pred_test_=classifier.predict(split_[1])
        
        hyperpar_list.append({'precision':round(precision_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2),'hyperparams':train_rf_})
        # array for features,precison,recall,f1
        precision=np.append(classifier.feature_importances_,round(precision_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        recall=np.append(precision,round(recall_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        f1=np.append(recall,round(f1_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        acc=np.append(f1,round(accuracy_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        # false positive, false negative, true negative, true positive 
        tn,fp,fn,tp = confusion_matrix(split_[3][:,0].astype('int'),y_pred_test_).ravel()
        false_positive=np.append(acc,fp)
        false_negative=np.append(false_positive,fn)
        true_negative=np.append(false_negative,tn)
        true_positive=np.append(true_negative,tp)
        # evaluation group by repository 
        repository=np.concatenate((split_[3][:,0].astype('int').reshape(len(split_[3][:,0].astype('int')),1), y_pred_test_.reshape(len(y_pred_test_),1),split_[3][:,1].astype('int').reshape(len(split_[3][:,1].astype('int')),1)), axis=1, out=None)
        npm=repository[repository[:,2] == 1]
        pypi=repository[repository[:,2] == 2]
        # precision, recall, f1 for npm 
        precision_npm=np.append(true_positive,round(precision_score(npm[:,0], npm[:,1])*100,2))
        recall_npm=np.append(precision_npm,round(recall_score(npm[:,0], npm[:,1])*100,2))
        f1_npm=np.append(recall_npm,round(f1_score(npm[:,0], npm[:,1])*100,2))
        acc_npm=np.append(f1_npm,round(accuracy_score(npm[:,0], npm[:,1])*100,2))
        # precision, recall, f1 for pypi
        precision_pypi=np.append(acc_npm,round(precision_score(pypi[:,0], pypi[:,1])*100,2))
        recall_pypi=np.append(precision_pypi,round(recall_score(pypi[:,0], pypi[:,1])*100,2))
        f1_pypi=np.append(recall_pypi,round(f1_score(pypi[:,0], pypi[:,1])*100,2))
        metrics=np.append(f1_pypi,round(accuracy_score(pypi[:,0], pypi[:,1])*100,2))
        eval[i]=metrics.tolist()

        
    # replace 0, with NaN 
    eval=eval.replace(0,np.nan)
    mean=eval.mean(axis=1)
    std=eval.std(axis=1)
    result=pd.concat([mean, std], axis=1)
    return(result, get_best_hyperparams(hyperpar_list))



def get_best_hyperparams(hyperparams_list):
    max_prec = hyperparams_list[0]['precision']
    final_hyperparam_set = hyperparams_list[0]['hyperparams']
    for e in hyperparams_list:
        if e['precision'] > max_prec:
            max_prec = e['precision']
            final_hyperparam_set = e['hyperparams']
    return final_hyperparam_set





# bayes opt grid search XGboost
def grid_xgb_py (regressors,labels):
    #function for the maximization of the target
    def xgb_cl_bo(max_depth,n_estimators,colsample_bytree,eta,gamma,min_child_weight):
        params_xgb={}
        params_xgb['max_depth'] = int(max_depth)
        params_xgb['n_estimators'] = int(n_estimators)
        params_xgb['colsample_bytree']=colsample_bytree
        params_xgb['min_child_weight'] = int(min_child_weight)
        params_xgb['eta']=eta
        params_xgb['gamma']=gamma
        classifier = xgb.XGBClassifier(random_state=123,n_estimators=params_xgb['n_estimators'],max_depth=params_xgb['max_depth'],gamma=params_xgb['gamma'],eta=params_xgb['eta'],colsample_bytree=params_xgb['colsample_bytree'],min_child_weight=params_xgb['min_child_weight'])
        #scores=cross_val_score(classifier,regressors,labels[:,0].astype('int'),cv=5,scoring='precision',n_jobs=-1)
        #target=scores.mean()
        scoring = {'rec': 'recall',
           'prec': 'precision' }
        scores = cross_validate(classifier,regressors,labels[:,0].astype('int'), scoring=scoring,
                         cv=5, return_train_score=True,n_jobs=-1)
        print('recall',round(scores['test_rec'].mean(),2)) 
        print('precision train',round(scores['train_prec'].mean(),2))  
        target=scores['test_prec'].mean()
        return (target)
    params_xgb ={
        'max_depth':(2, 4),
        'n_estimators':(64,256), 
        'min_child_weight':(8,16), 
        'gamma':(0.6,1.2),
        'eta':(0.08,0.16),
        'colsample_bytree':(0.1,0.3)
    }
    xgb_bo = BayesianOptimization(xgb_cl_bo, params_xgb, random_state=111,verbose=1)
    xgb_bo.maximize(init_points=25, n_iter=5)
    print(xgb_bo.max)
    params_xgb = xgb_bo.max['params']
    params_xgb={}
    params_xgb['n_estimators']= int(xgb_bo.max["params"]["n_estimators"])
    params_xgb["max_depth"] = int(xgb_bo.max["params"]["max_depth"])
    params_xgb['min_child_weight']= int(xgb_bo.max["params"]["min_child_weight"])
    params_xgb['eta']=xgb_bo.max['params']['eta']
    params_xgb['gamma']=xgb_bo.max['params']['gamma']
    params_xgb['colsample_bytree']=xgb_bo.max['params']['colsample_bytree']
    #print(params_tree)
    return (params_xgb) 



'''
Decision Tree

'''


def evaluation_decision_tree(database):
    
    database = database.loc[:, ~database.columns.str.contains('^Unnamed')]
    database['Package Repository'] = np.where(database['Package Repository'] == "NPM", 1, 2)
    # Dict to set set of params and related precision
    hyperpar_list = []
    # define a list for the evaluation metrics 
    evaluation=['precision','recall','f1', 'accuracy','false positive','false negative','true negative','true positive','precision_npm','recall_npm','f1_npm', 'acc_npm','precision_pypi','recall_pypi','f1_pypi','acc_pypi']
    # define a list for the features 
    #f = [c for c in database.columns if c not in ['Malicious','Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','Package Repository','Package Name']]#,'Number of lines in metadata','Number of Words in metadata'
    database = database.loc[:, ~database.columns.str.contains('^Unnamed')]

    f = [c for c in database.columns if c not in ['Malicious','Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','Unnamed: 0.1.1.1','Package Repository','Package Name']]#,'Number of lines in metadata','Number of Words in metadata'
    # initialize the dataframe
    eval = pd.DataFrame(data=None, index=[y for x in [f,evaluation] for y in x])
    random_split=[123,333,567,999,876,371,459,111,902,724]
    for i in range(0,10):
        #split
        split_=split_training_testing(database[database['Package Repository']!=3], test_size=0.2, random=random_split[i])
        # optimization of the hyperparameters with cross-validation in the train set 
        train_rf_=grid_tree(split_[0], split_[2])
        # fit the model with the best hyperparameters 
        classifier = DecisionTreeClassifier(random_state=123,criterion=train_rf_['criterion'],max_depth=train_rf_['max_depth'],max_features=train_rf_['max_features'],min_samples_leaf=train_rf_['min_sample_leaf'],min_samples_split=train_rf_['min_sample_split'])
        classifier.fit(split_[0], split_[2][:,0].astype('int'))
        #predict on test data 
        y_pred_test_=classifier.predict(split_[1])
        hyperpar_list.append({'precision':round(precision_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2),'hyperparams':train_rf_})
        # array for features,precison,recall,f1
        precision=np.append(classifier.feature_importances_,round(precision_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        recall=np.append(precision,round(recall_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        f1=np.append(recall,round(f1_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        acc=np.append(f1,round(accuracy_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        # false positive, false negative, true negative, true positive 
        tn,fp,fn,tp = confusion_matrix(split_[3][:,0].astype('int'),y_pred_test_).ravel()
        false_positive=np.append(acc,fp)
        false_negative=np.append(false_positive,fn)
        true_negative=np.append(false_negative,tn)
        true_positive=np.append(true_negative,tp)
        # evaluation group by repository 
        repository=np.concatenate((split_[3][:,0].astype('int').reshape(len(split_[3][:,0].astype('int')),1), y_pred_test_.reshape(len(y_pred_test_),1),split_[3][:,1].astype('int').reshape(len(split_[3][:,1].astype('int')),1)), axis=1, out=None)
        npm=repository[repository[:,2] == 1]
        pypi=repository[repository[:,2] == 2]
        rubygems=repository[repository[:,2] == 3]
        # precision, recall, f1 for npm 
        precision_npm=np.append(true_positive,round(precision_score(npm[:,0], npm[:,1])*100,2))
        recall_npm=np.append(precision_npm,round(recall_score(npm[:,0], npm[:,1])*100,2))
        f1_npm=np.append(recall_npm,round(f1_score(npm[:,0], npm[:,1])*100,2))
        acc_npm=np.append(f1_npm,round(accuracy_score(npm[:,0], npm[:,1])*100,2))
        # precision, recall, f1 for pypi
        precision_pypi=np.append(acc_npm,round(precision_score(pypi[:,0], pypi[:,1])*100,2))
        recall_pypi=np.append(precision_pypi,round(recall_score(pypi[:,0], pypi[:,1])*100,2))
        f1_pypi=np.append(recall_pypi,round(f1_score(pypi[:,0], pypi[:,1])*100,2))
        metrics=np.append(f1_pypi,round(accuracy_score(pypi[:,0], pypi[:,1])*100,2))
        eval[i]=metrics.tolist()
    # replace 0, with NaN 
    eval=eval.replace(0,np.nan)
    mean=eval.mean(axis=1)
    std=eval.std(axis=1)
    result=pd.concat([mean, std], axis=1)
    return(result, get_best_hyperparams(hyperpar_list))

# bayes opt grid search decision tree 
def grid_tree (regressors,labels):
    # grid for the quality of the split 
    criteria=['gini', 'entropy', 'log_loss'] # 0,1,2
    number_features=['sqrt','log2',None]
    #function for the maximization of the target
    def tree_cl_bo(max_depth, max_features,criterion,min_sample_leaf,min_sample_split):
        params_tree={}
        params_tree['max_depth'] = int(max_depth)
        params_tree['max_features'] = number_features[int(max_features)]
        params_tree['criterion']=criteria[int(criterion)]
        params_tree['min_sample_leaf']=int(min_sample_leaf)
        params_tree['min_sample_split']=int(min_sample_split)
        classifier = DecisionTreeClassifier(random_state=123,criterion=params_tree['criterion'],max_depth=params_tree['max_depth'],min_samples_leaf=params_tree['min_sample_leaf'],max_features=params_tree['max_features'],min_samples_split=params_tree['min_sample_split'])
        #scores=cross_val_score(classifier,regressors,labels[:,0].astype('int'),cv=5,scoring='precision',n_jobs=-1)
        #target=scores.mean()
        scoring = {'rec': 'recall',
           'prec': 'precision'}
        scores = cross_validate(classifier,regressors,labels[:,0].astype('int'), scoring=scoring,
                         cv=5, return_train_score=True,n_jobs=-1)
        print('recall',round(scores['test_rec'].mean(),2))
        print('precision train',round(scores['train_prec'].mean(),2))   
        target=scores['test_prec'].mean()
        return (target)
    params_tree ={
        'max_depth':(2, 4),
        'max_features':(0,2.99), 
        'criterion':(0,2.99), # int 0,1,2
        'min_sample_leaf':(4,8),
        'min_sample_split':(6,16)
    }
    tree_bo = BayesianOptimization(tree_cl_bo, params_tree, random_state=111)
    tree_bo.maximize(init_points=25, n_iter=5)
    print(tree_bo.max)
    params_tree = tree_bo.max['params']
    params_tree={}
    params_tree["max_features"]=number_features[int(tree_bo.max["params"]["max_features"])]
    params_tree["max_depth"] = int(tree_bo.max["params"]["max_depth"])
    params_tree['criterion']= criteria[int(tree_bo.max["params"]["criterion"])]
    params_tree['min_sample_leaf']=int(tree_bo.max['params']['min_sample_leaf'])
    params_tree['min_sample_split']=int(tree_bo.max['params']['min_sample_split'])
    #print(params_tree)
    return (params_tree)  




'''
Random Forest

'''

def evaluation_random_forest(database):
    database = database.loc[:, ~database.columns.str.contains('^Unnamed')]
    database['Package Repository'] = np.where(database['Package Repository'] == "NPM", 1, 2)

    # Dict to set set of params and related precision
    hyperpar_list = [] 
    # define a list for the evaluation metrics 
    evaluation=['precision','recall','f1', 'accuracy','false positive','false negative','true negative','true positive','precision_npm','recall_npm','f1_npm', 'acc_npm','precision_pypi','recall_pypi','f1_pypi','acc_pypi']
    # define a list for the features 
    f = [c for c in database.columns if c not in ['Malicious','Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1','Package Repository','Package Name']]#,'Number of lines in metadata','Number of Words in metadata'
    # initialize the dataframe
    eval = pd.DataFrame(data=None, index=[y for x in [f,evaluation] for y in x])
    random_split=[123,333,567,999,876,371,459,111,902,724]
    for i in range(0,10):
        #split
        split_=split_training_testing(database[database['Package Repository']!=3], test_size=0.2, random=random_split[i])
        # optimization of the hyperparameters with cross-validation in the train set 
        train_rf_=grid_rf(split_[0], split_[2])
        # fit the model with the best hyperparameters 
        classifier = RandomForestClassifier(random_state=123,criterion=train_rf_['criterion'],n_estimators=train_rf_['n_estimators'],max_depth=train_rf_['max_depth'],max_features=train_rf_['max_features'],min_samples_leaf=train_rf_['min_sample_leaf'],min_samples_split=train_rf_['min_sample_split'],max_samples=train_rf_['max_samples'])
        classifier.fit(split_[0], split_[2][:,0].astype('int'))
        #predict on test data 
        y_pred_test_=classifier.predict(split_[1])

        hyperpar_list.append({'precision':round(precision_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2),'hyperparams':train_rf_})
        # array for features,precison,recall,f1
        precision=np.append(classifier.feature_importances_,round(precision_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        recall=np.append(precision,round(recall_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        f1=np.append(recall,round(f1_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        acc=np.append(f1,round(accuracy_score(split_[3][:,0].astype('int'), y_pred_test_)*100,2))
        # false positive, false negative, true negative, true positive 
        tn,fp,fn,tp = confusion_matrix(split_[3][:,0].astype('int'),y_pred_test_).ravel()
        false_positive=np.append(acc,fp)
        false_negative=np.append(false_positive,fn)
        true_negative=np.append(false_negative,tn)
        true_positive=np.append(true_negative,tp)
        # evaluation group by repository 
        repository=np.concatenate((split_[3][:,0].astype('int').reshape(len(split_[3][:,0].astype('int')),1), y_pred_test_.reshape(len(y_pred_test_),1),split_[3][:,1].astype('int').reshape(len(split_[3][:,1].astype('int')),1)), axis=1, out=None)
        npm=repository[repository[:,2] == 1]
        pypi=repository[repository[:,2] == 2]
        rubygems=repository[repository[:,2] == 3]
        # precision, recall, f1 for npm 
        precision_npm=np.append(true_positive,round(precision_score(npm[:,0], npm[:,1])*100,2))
        recall_npm=np.append(precision_npm,round(recall_score(npm[:,0], npm[:,1])*100,2))
        f1_npm=np.append(recall_npm,round(f1_score(npm[:,0], npm[:,1])*100,2))
        acc_npm=np.append(f1_npm,round(accuracy_score(npm[:,0], npm[:,1])*100,2))
        # precision, recall, f1 for pypi
        precision_pypi=np.append(acc_npm,round(precision_score(pypi[:,0], pypi[:,1])*100,2))
        recall_pypi=np.append(precision_pypi,round(recall_score(pypi[:,0], pypi[:,1])*100,2))
        f1_pypi=np.append(recall_pypi,round(f1_score(pypi[:,0], pypi[:,1])*100,2))
        metrics=np.append(f1_pypi,round(accuracy_score(pypi[:,0], pypi[:,1])*100,2))
        
        eval[i]=metrics.tolist()
    # replace 0, with NaN 
    eval=eval.replace(0,np.nan)
    mean=eval.mean(axis=1)
    std=eval.std(axis=1)
    result=pd.concat([mean, std], axis=1)
    return(result, get_best_hyperparams(hyperpar_list))


# bayes opt grid search RANDOM FOREST
def grid_rf (regressors,labels):
    # grid for the quality of the split 
    criteria=['gini', 'entropy', 'log_loss'] # 0,1,2
    number_features=['sqrt','log2',None]
    #function for the maximization of the target
    def rf_cl_bo(max_depth, max_features,n_estimators,criterion,min_sample_leaf,min_sample_split,max_samples):
        params_rf={}
        params_rf['max_depth'] = int(max_depth)
        params_rf['max_features'] = number_features[int(max_features)]
        params_rf['criterion']=criteria[int(criterion)]
        params_rf['n_estimators'] = int(n_estimators)
        params_rf['min_sample_leaf']=int(min_sample_leaf)
        params_rf['min_sample_split']=int(min_sample_split)
        params_rf['max_samples']=max_samples
        classifier = RandomForestClassifier(random_state=123,criterion=params_rf['criterion'],n_estimators=params_rf['n_estimators'],max_depth=params_rf['max_depth'],min_samples_leaf=params_rf['min_sample_leaf'],max_features=params_rf['max_features'],max_samples=params_rf['max_samples'],min_samples_split=params_rf['min_sample_split'])
        #scores=cross_val_score(classifier,regressors,labels[:,0].astype('int'),cv=5,scoring='precision',n_jobs=-1)
        scoring = {'rec': 'recall',
           'prec': 'precision'}
        scores = cross_validate(classifier,regressors,labels[:,0].astype('int'), scoring=scoring,
                         cv=5, return_train_score=True,n_jobs=-1)
        print('recall',round(scores['test_rec'].mean(),2))  
        print('precision train',round(scores['train_prec'].mean(),2)) 
        target=scores['test_prec'].mean()
        return (target)
    params_rf ={
        'max_depth':(2, 4),
        'max_features':(0,2.99),
        'n_estimators':(64,256), 
        'criterion':(0,2.99), # int 0,1,2
        'min_sample_leaf':(4,8),
        'min_sample_split':(6,16),
        'max_samples':(0.1,1)
    }
    rf_bo = BayesianOptimization(rf_cl_bo, params_rf, random_state=111)
    rf_bo.maximize(init_points=25, n_iter=5)
    print(rf_bo.max)
    params_rf = rf_bo.max['params']
    params_rf={}
    params_rf['n_estimators']= int(rf_bo.max["params"]["n_estimators"])
    params_rf["max_features"]=number_features[int(rf_bo.max["params"]["max_features"])]
    params_rf["max_depth"] = int(rf_bo.max["params"]["max_depth"])
    params_rf['criterion']= criteria[int(rf_bo.max["params"]["criterion"])]
    params_rf['min_sample_leaf']=int(rf_bo.max['params']['min_sample_leaf'])
    params_rf['min_sample_split']=int(rf_bo.max['params']['min_sample_split'])
    params_rf['max_samples']=rf_bo.max['params']['max_samples']
    #print(params_tree)
    return (params_rf)  