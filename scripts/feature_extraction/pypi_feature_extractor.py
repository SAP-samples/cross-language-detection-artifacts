from typing import final
import nltk
import json, os, tarfile
import utilities_functions
import pandas as pd
import numpy as np
import statistics
import stat

from pygments.lexers import PythonLexer
from pygments.token import Token
from pathlib import Path

from functools import reduce
from urlextract import URLExtract

class PyPI_Feature_Extractor:


    def __init__(self) :

        
        # extensions 
        # classes: source code, compiled code, packages code, image, video, audio, archive, font, apps, document, data, web, security, database. 
        self.classes = ['bat',	'bz2',	'c', 'cert','conf','cpp' ,'crt', 'css',	'csv', 'deb' ,'erb','gemspec', 'gif', 'gz', 'h', 'html', 'ico' ,'ini' ,'jar', 'java', 'jpg', 'js', 'json', 'key' ,'m4v' ,'markdown' ,'md' ,'pdf', 'pem', 'png', 'ps', 'py',	'rb', 'rpm', 'rst','sh'	,'svg',	'toml',	'ttf',	'txt','xml', 'yaml', 'yml', 'eot', 'exe', 'jpeg', 'properties',	'sql',	'swf',	'tar',	'woff', 'woff2', 'aac','bmp', 'cfg' ,'dcm', 'dll', 'doc', 'flac','flv',	'ipynb', 'm4a', 'mid', 'mkv', 'mp3', 'mp4', 'mpg', 'ogg','otf', 'pickle', 'pkl' ,'psd',	'pxd' ,'pxi', 'pyc', 'pyx', 'r', 'rtf',	'so', 'sqlite' ,'tif',	'tp', 'wav', 'webp' ,'whl', 'xcf', 'xz', 'zip' ,'mov' ,'wasm', 'webm']
        # stopwords 
        nltk.download('stopwords')
        self.stopwords = set(nltk.corpus.stopwords.words('english'))
       
        # dangerous token 
        with open('resources/dangerous_tokens.json', 'r') as file:
            self.dangerous_token = json.load(file)


    def extract_features(self, path: str) -> pd.DataFrame:
        '''
        Executes the whole pipeline for the extraction of
        the features from the packages contained in the provided path
        
        Input: Path to the set of samples to be classified
        Output: Dataframe containing extracted data for each package
        '''

        self.path_to_scan = path
        self.unzip_packages()
        py_files_df = self.extract_features_from_py()[0]
        
        setup_files_df = self.extract_features_from_py()[1]
        extensions_files_df =  self.count_package_files_extension()

        dfs = [py_files_df, setup_files_df,extensions_files_df]
        final_df = reduce(lambda  left,right: pd.merge(left,right,on=['Package Name'],
                                            how='outer'), dfs)
        final_df = self.extraction(final_df, utilities_functions.gen_language_4,4,utilities_functions.gen_language_4,4)
        final_df.to_csv("pypi_feature_extracted.csv", encoding='utf-8', index=False)
        return final_df


    def unzip_packages(self) -> None: 
        '''
        Unzips the .tar.gz file of each pyPI package
        '''
        
        for root, dirs, files in os.walk(self.path_to_scan):
            for file in files:
                if file.endswith(".tar.gz"):
                    if os.path.getsize(os.path.join(self.path_to_scan,file)) > 0:
                    
                        output_dir="".join((self.path_to_scan,"/",file.split(".tar.gz")[0]))
                        print(f"[*] Processing {file}")
                        pkg_file = tarfile.open(os.path.join(self.path_to_scan,file))
                        pkg_file.extractall(output_dir)
                        #os.chmod(output_dir, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)

                        pkg_file.close()
         
    def extract_features_from_py(self) -> pd.DataFrame:

        '''
        Extract the features from the list of paths containing JS files
        
        Input: list of path and string for the specific extension, .js extension, stopwords to be removed.
        Output: pandas dataframe  
        
        '''

        files_path = utilities_functions.find_files_of_ext(self.path_to_scan, ".py")
        #initialize the lists 
        Package=list()
        version=list()
        jsfile=list()
        strings=list()
        #strings_entire=list()
        identifiers=list()
        sospicious_token=list()
        lines=list()
        plus_ratio=list()
        equal_ratio=list()
        square_ratio=list()
        Base64=list()
        ip=list()
        code=list()
        #initialize pandas 
        db=pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)
        for i in range(len(files_path)):
            # initialize the list for the puntuactions and operators token 
            operator=[]
            punctuation=[]
            other=[]
            id=[]
            strs=[]
            p=Path(files_path[i])
            # package name ###### change the path here adapt to the new repo
            
            package_name=p.parts[2]
            # name of the file
            js=p.parts[-1]
            file = open(files_path[i],"r",encoding="utf8",errors='ignore',newline='\n')
            # convert to string 
            data=file.read()
            # apply the lexer specific for language
            lexer=PythonLexer(stripnl=False,ensurenl=False)
            token_source = lexer.get_tokens(data)
            for token in token_source:
                    if token[0] in Token.Operator:
                        operator.append(token[1])
                    elif token[0] in Token.Punctuation:
                        punctuation.append(token[1])
                    elif token[0] in Token.Name:
                        id.append(token[1])
                    elif (token[0] in Token.Literal.String.Single or token[0] in Token.Literal.String.Double or token[0] in Token.Literal.String.Affix or token[0] in Token.Literal.String.Backtick or token[0] in Token.Literal.String.Char or token[0] in Token.Literal.String.Delimiter or token[0] in Token.Literal.String.Doc or token[0] in Token.Literal.String.Escape or token[0] in Token.Literal.String.Heredoc or token[0] in Token.Literal.String.Interpol or token[0] in Token.Literal.String.Other):
                        strs.append(token[1]) 
                    else:
                        other.append(token[1]) 
            with open(files_path[i],"r",encoding="utf8",errors='ignore') as fp:
                num_lines = sum(1 for line in fp)
                size = fp.seek(0, os.SEEK_END)
                size+=1
            id = [s.replace("'", '') for s in id]
            id = [s.replace('"', '') for s in id]
            id_=' '.join(id)
            equalities=operator.count('=')/size
            plus=operator.count('+')/size
            Lbrackets=punctuation.count('[')/size
            count_base64=0
            count_IP=0
            byte=0
            for value in range(0,len(strs)):
                count_base64+=len(utilities_functions.contains_base64(strs[value]))
                count_IP+=len(utilities_functions.contains_IPAddress(strs[value]))
                # contains_dangerous_token --> sospicious list
                byte+=len(utilities_functions.contains_dangerous_token(strs[value],self.dangerous_token))
            strs = [s.replace("'", '') for s in strs]
            strs = [s.replace('"', '') for s in strs]
            #strings_entire.append(strs)
            string=' '.join(strs).split()
            #remove stopwords
            string=list(set(strs)-self.stopwords) 
            string_=' '.join(string)
            file.close()
            fp.close()
            #append result to list
            code.append(data)
            Package.append(package_name)
            jsfile.append(js)
            sospicious_token.append(byte)
            lines.append(num_lines)
            plus_ratio.append(plus)
            equal_ratio.append(equalities)
            square_ratio.append(Lbrackets)
            identifiers.append(id_)
            Base64.append(count_base64)
            ip.append(count_IP)
            strings.append(string_)
        # assign to pandas dataframe
        
        db['Package Name']=Package
        db['.py']=jsfile
        db['sospicious token']=sospicious_token
        db['lines']=lines
        db['equal ratio']=equal_ratio
        db['plus ratio']=plus_ratio
        db['bracket ratio']=square_ratio
        db['identifiers']=identifiers
        db['base64']=Base64
        db['IP']=ip
        db['strings']=strings
        #db['strings entire']=strings_entire
        db['code']=code
        # returns two dataframe one for all .py files and one only for setup.py file
        setup_db=db[db['.py']=='setup.py']
        db.drop(db.index[db['.py']=='setup.py'],inplace=True)
        return (self.merge_py_of_same_package(db),self.merge_setup_of_same_package(setup_db))


    def merge_py_of_same_package(self, database: pd.DataFrame) -> pd.DataFrame:
        p_database= database.groupby(['Package Name'], as_index=False)['code'].agg('\n'.join)
        p_database['Number of words'] = p_database["code"].apply(lambda n: len(n.split()))
        l_database = database.groupby(['Package Name'], as_index=False)['lines'].sum()
        plus_mean= database.groupby(['Package Name'], as_index=False)['plus ratio'].mean()
        plus_mean = plus_mean.rename(columns={"plus ratio": "plus ratio mean"})
        plus_max= database.groupby(['Package Name'], as_index=False)['plus ratio'].max()
        plus_max = plus_max.rename(columns={"plus ratio": "plus ratio max"})    
        plus_std= database.groupby(['Package Name'], as_index=False)['plus ratio'].std()
        plus_std = plus_std.rename(columns={"plus ratio": "plus ratio std"})    
        plus_q3= database.groupby(['Package Name'], as_index=False)['plus ratio'].quantile(0.75)
        plus_q3 = plus_q3.rename(columns={"plus ratio": "plus ratio q3"})    
        eq_mean= database.groupby(['Package Name'], as_index=False)['equal ratio'].mean()
        eq_mean = eq_mean.rename(columns={"equal ratio": "equal ratio mean"})    
        eq_max= database.groupby(['Package Name'], as_index=False)['equal ratio'].max()
        eq_max = eq_max.rename(columns={"equal ratio": "equal ratio max"})
        eq_std= database.groupby(['Package Name'], as_index=False)['equal ratio'].std()
        eq_std = eq_std.rename(columns={"equal ratio": "equal ratio std"})
        eq_q3= database.groupby(['Package Name'], as_index=False)['equal ratio'].quantile(0.75)
        eq_q3 = eq_q3.rename(columns={"equal ratio": "equal ratio q3"})
        bracket_mean= database.groupby(['Package Name'], as_index=False)['bracket ratio'].mean()
        bracket_mean = bracket_mean.rename(columns={"bracket ratio": "bracket ratio mean"})
        bracket_max= database.groupby(['Package Name'], as_index=False)['bracket ratio'].max()
        bracket_max = bracket_max.rename(columns={"bracket ratio": "bracket ratio max"})
        bracket_std= database.groupby(['Package Name'], as_index=False)['bracket ratio'].std()
        bracket_std = bracket_std.rename(columns={"bracket ratio": "bracket ratio std"})
        bracket_q3= database.groupby(['Package Name'], as_index=False)['bracket ratio'].quantile(0.75)
        bracket_q3 = bracket_q3.rename(columns={"bracket ratio": "bracket ratio q3"})
        base = database.groupby(['Package Name'], as_index=False)['base64'].sum()
        ip = database.groupby(['Package Name'], as_index=False)['IP'].sum()
        sospicious = database.groupby(['Package Name'], as_index=False)['sospicious token'].sum()
        string = database.groupby(['Package Name'], as_index=False)['strings'].agg(' '.join)
        #string_entire = database.groupby(['Package Name'], as_index=False)['strings entire'].agg(lambda x: list(flatten(x)))
        identifier = database.groupby(['Package Name'], as_index=False)['identifiers'].agg(' '.join)
        #p_database['Number of files']=database.groupby(['Package Name', 'version'], as_index=False)['Package Name'].count()['Package Name']
        # merge p_database and l_dataabse
        data = [p_database,l_database,plus_mean,plus_max,plus_std,plus_q3,eq_mean,eq_max,eq_std,eq_q3,bracket_mean,bracket_max,bracket_std,bracket_q3,base,ip,sospicious,string,identifier]
        #merge all DataFrames into one
        final_database = reduce(lambda  left,right: pd.merge(left,right,on=['Package Name'], how='outer'), data)
        final_database.drop('code',axis=1,inplace=True)
        final_database.columns=['Package Name','Number of words','lines','plus ratio mean','plus ratio max','plus ratio std','plus ratio q3','eq ratio mean','eq ratio max','eq ratio std','eq ratio q3','bracket ratio mean','bracket ratio max','bracket ratio std','bracket ratio q3','base64','IP','sospicious token','strings','identifiers']
        return (final_database)

    def merge_setup_of_same_package(self,database):
        p_database= database.groupby(['Package Name'], as_index=False)['code'].agg('\n'.join)
        p_database['Number of words'] = p_database["code"].apply(lambda n: len(n.split()))
        l_database = database.groupby(['Package Name'], as_index=False)['lines'].sum()
        base = database.groupby(['Package Name'], as_index=False)['base64'].sum()
        ip = database.groupby(['Package Name'], as_index=False)['IP'].sum()
        sospicious = database.groupby(['Package Name'], as_index=False)['sospicious token'].sum()
        string = database.groupby(['Package Name'], as_index=False)['strings'].agg(' '.join)
        #string_entire = database.groupby(['Package Name'], as_index=False)['strings entire'].agg(lambda x: list(flatten(x)))
        identifier = database.groupby(['Package Name'], as_index=False)['identifiers'].agg(' '.join)
        #p_database['Number of files']=database.groupby(['Package Name', 'version'], as_index=False)['Package Name'].count()['Package Name']
        # merge p_database and l_dataabse
        data = [p_database,l_database,base,ip,sospicious,string,identifier]
        #merge all DataFrames into one
        final_database = reduce(lambda  left,right: pd.merge(left,right,on=['Package Name'], how='outer'), data)
        final_database.drop('code',axis=1,inplace=True)
        final_database.columns=['Package Name','Number of words','lines','base64','IP','sospicious token','strings','identifiers']
        return (final_database)

        #### 
        # classes: list of extension we are looking for  
    def count_package_files_extension(self) -> pd.DataFrame:
        '''
        function for extraction number of files with a given extension inside a given package  
        root: folder that contains the malicious packages
        classes: list of extension we are looking for
        function to add a point before the list of extensions
        '''
        #initialize the lists 
        Package=list()
        extension=list()
        #initialize pandas 
        db=pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)
        # for each extension
        for i in range(0,len(self.classes)):
                #extract the extension we are interested in:
                ext='.'+self.classes[i]
                files_path=utilities_functions.find_files_of_ext(self.path_to_scan,ext)
                # for each file path
                for j in range(len(files_path)):
                        # extract the path
                        p=Path(files_path[j])
                        # package name ##### change the path here 
                        if "tar.gz" not in p.parts[-1]:
                                package_name=p.parts[2]
                                
                                # version name
                                Package.append(package_name)
                                extension.append(ext)
       
        db['Package Name']=Package
        db['extension']=extension
        # count frequency of extension, grouped by package name and version
        db=db.groupby(['Package Name', 'extension']).size().unstack(fill_value=0)
        # for each package keep only the last version
        db=db.groupby('Package Name').last()
        
        def add_to_beginning(s, start='.'):
                return start + s
        extensions = list(map(add_to_beginning, self.classes))
        #select extensions not founded in the initial list
        f = [c for c in extensions if c not in db.columns]
        #add them to the dataframe
        db = pd.concat([db,pd.DataFrame(columns = f)])
        # fill Nan with 0
        db[f] = db[f].fillna(0)
        # order the column 
        db=db[extensions]
        db.reset_index(inplace=True)
        db =db.rename(columns = {'index':'Package Name'})
        return (db)

    def extraction(self, database,alphabetic_string,base_string,alphabetic_id,base_id):
        extractor = URLExtract()
        # repository for Pypi 
        database['repository'] = pd.Series([2 for x in range(len(database.index))])
        f = [c for c in database.columns if c not in ['strings_x','identifiers_x','strings_y','identifiers_y']]
        database[f] = database[f].fillna(0)
        # reset index 
        database.index=range(0,len(database))
        #extractor.update() For updating TLDs list 
        # define code to inspect and name of the package  
        source_code_strings=database['strings_x']
        source_code_identifiers=database['identifiers_x']
        metadata_strings=database['strings_y']
        metadata_identifiers=database['identifiers_y']
        name=database['Package Name']
        repository=database['repository']
        check_metadata_strings=metadata_strings.isna()
        check_metadata_identifiers=metadata_identifiers.isna()
        check_source_code_strings=source_code_strings.isna()
        check_source_code_identifiers=source_code_identifiers.isna()
        #initilize lists: one value for each package.
        # source code shannon's features 
        q3_id_sc=[]
        q3_str_sc=[]
        m_id_sc=[]
        m_str_sc=[]
        dev_id_sc=[]
        dev_str_sc=[]
        maximum_id_sc=[]
        maximum_str_sc=[]
        flat_id_sc=[]
        flat_string_sc=[]
        count_url_sc=[]
        obf_id_sc=[]
        obf_string_sc=[]
        # metadata shannon's features 
        q3_id_md=[]
        q3_str_md=[]
        m_id_md=[]
        m_str_md=[]
        dev_id_md=[]
        dev_str_md=[]
        maximum_id_md=[]
        maximum_str_md=[]
        flat_id_md=[]
        flat_string_md=[]
        count_url_md=[]
        obf_id_md=[]
        obf_string_md=[]
        # installation script feature in metadata
        installation=[]
        #db=pd.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False)
        for i in range(len(database)): 
                
                # select the entry points specific for each language
                if repository[i]==3:
                        install=['extensions']
                elif repository[i]==2:
                        install=['install']
                else: 
                        install=['postinstall','preinstall','install']
                # source code
                if check_source_code_strings[i]==False:
                        # string 
                        string_sourcecode=source_code_strings[i]
                        # create a list of strings from a unique string
                        string=string_sourcecode.split()
                else: 
                        string=[]
                if check_source_code_identifiers[i]==False:
                        # identifiers
                        identifiers_sourcecode=source_code_identifiers[i]
                        # create a list of identifiers from a unique string
                        identifiers=identifiers_sourcecode.split()
                else: 
                        identifiers=[]
                # apply the generalization language
                generalization_str=[]
                generalization_id=[]
                # identifiers 
                for h in range(0,len(identifiers)):
                        gen=alphabetic_id(identifiers[h])
                        generalization_id.append(gen)
                obf_sc=utilities_functions.obfuscation(generalization_id,symbols=['u','d','l','s'])
                # strings
                url_sc=0
                for k in range(0,len(string)):
                        try:
                                url_sc+=len(extractor.find_urls(string[k]))
                        except:
                                url_sc += len(utilities_functions.contains_URL(string[k]))
                        gen=alphabetic_string(string[k])
                        generalization_str.append(gen)  
                obf_sc_str=utilities_functions.obfuscation(generalization_str,symbols=['u','d','l','s'])
                # apply shannon entropy   
                shannon_str=[]
                shannon_id=[]
                # identifiers 
                for w in range(0,len(generalization_id)):
                        shan=utilities_functions.shannon_entropy(generalization_id[w],base_id)
                        shannon_id.append(shan)
                # strings
                for y in range(0,len(generalization_str)):
                        shan=utilities_functions.shannon_entropy(generalization_str[y],base_string)
                        shannon_str.append(shan) 
                # remove shannon values which are equal to 0
                #shannon_str_no0 = list(filter(lambda x: abs(x) != 0,shannon_str))
                #shannon_id_no0 = list(filter(lambda x: abs(x) != 0, shannon_id))
                null_string_sc=len(list(filter(lambda x: abs(x) == 0,shannon_str)))
                null_id_sc=len(list(filter(lambda x: abs(x) == 0, shannon_id)))
                #shannon_str=shannon_str_no0
                #shannon_id=shannon_id_no0
                if len(shannon_str)>=1:
                        mean_str=statistics.mean(shannon_str)
                        max_str=max(shannon_str)
                        quart_str=np.quantile(shannon_str,0.75)
                else:
                        mean_str=0
                        max_str=0
                        quart_str=0
                if len(shannon_str)>1:
                        std_str=np.std(shannon_str)
                else:
                        std_str=0    
                if len(shannon_id)>=1:
                        mean_id=statistics.mean(shannon_id)
                        max_id=max(shannon_id)
                        quart_id=np.quantile(shannon_id, 0.75)
                else:
                        mean_id=0
                        max_id=0
                        quart_id=0
                if len(shannon_id)>1:
                        std_id=np.std(shannon_id)
                else:
                        std_id=0
                m_str_sc.append(mean_str)
                dev_str_sc.append(std_str)
                maximum_str_sc.append(max_str)
                q3_str_sc.append(quart_str)
                m_id_sc.append(mean_id)
                dev_id_sc.append(std_id)
                maximum_id_sc.append(max_id)
                q3_id_sc.append(quart_id)
                flat_id_sc.append(null_id_sc)
                flat_string_sc.append(null_string_sc)
                count_url_sc.append(url_sc)
                obf_id_sc.append(obf_sc)
                obf_string_sc.append(obf_sc_str)
                #metadata analysis 
                # string
                if check_metadata_strings[i]==False: 
                        string_metadata=metadata_strings[i]
                        # create a list of strings from a unique string
                        string_md=string_metadata.split()
                else: 
                        string_md=[]
                # identifiers
                if check_metadata_identifiers[i]==False:
                        identifiers_metadata=metadata_identifiers[i]
                        # create a list of identifiers from a unique string
                        identifiers_md=identifiers_metadata.split()
                        if any(f in identifiers_md for f in install)==True:
                                install_script=1
                        else:
                                install_script=0
                else: 
                        identifiers_md=[]
                        install_script=0
                # apply the generalization language
                generalization_str_md=[]
                generalization_id_md=[]
                # identifiers 
                for h in range(0,len(identifiers_md)):
                        gen=alphabetic_id(identifiers_md[h])
                        generalization_id_md.append(gen)
                obf_md=utilities_functions.obfuscation(generalization_id_md,symbols=['u','d','l','s'])
                # strings
                url_md=0
                for k in range(0,len(string_md)):
                        try:
                                url_sc+=len(extractor.find_urls(string_md[k]))
                        except:
                                url_sc += len(utilities_functions.contains_URL(string_md[k]))
                        gen=alphabetic_string(string_md[k])
                        generalization_str_md.append(gen)  
                obf_md_str=utilities_functions.obfuscation(generalization_str_md,symbols=['u','d','l','s'])
                # apply shannon entropy   
                shannon_str_md=[]
                shannon_id_md=[]
                # identifiers 
                for w in range(0,len(generalization_id_md)):
                        shan=utilities_functions.shannon_entropy(generalization_id_md[w],base_id)
                        shannon_id_md.append(shan)
                # strings
                for y in range(0,len(generalization_str_md)):
                        shan=utilities_functions.shannon_entropy(generalization_str_md[y],base_string)
                        shannon_str_md.append(shan) 
                # remove shannon values which are equal to 0
                #shannon_str_md_no0 = list(filter(lambda x: abs(x) != 0,shannon_str_md))
                #shannon_id_md_no0 = list(filter(lambda x: abs(x) != 0, shannon_id_md))
                null_id_md=len(list(filter(lambda x: abs(x) == 0, shannon_id_md)))
                null_string_md=len(list(filter(lambda x: abs(x) == 0,shannon_str_md)))
                #shannon_str_md=shannon_str_md_no0
                #shannon_id_md=shannon_id_md_no0
                if len(shannon_str_md)>=1:
                        mean_str_md=statistics.mean(shannon_str_md)
                        max_str_md=max(shannon_str_md)
                        quart_str_md=np.quantile(shannon_str_md,0.75)
                else:
                        mean_str_md=0
                        max_str_md=0
                        quart_str_md=0
                if len(shannon_str_md)>1:
                        std_str_md=np.std(shannon_str_md)
                else:
                        std_str_md=0    
                if len(shannon_id_md)>=1:
                        mean_id_md=statistics.mean(shannon_id_md)
                        max_id_md=max(shannon_id_md)
                        quart_id_md=np.quantile(shannon_id_md, 0.75)
                else:
                        mean_id_md=0
                        max_id_md=0
                        quart_id_md=0
                if len(shannon_id_md)>1:
                        std_id_md=np.std(shannon_id_md)
                else:
                        std_id_md=0
                installation.append(install_script)  
                m_str_md.append(mean_str_md)
                dev_str_md.append(std_str_md)
                maximum_str_md.append(max_str_md)
                q3_str_md.append(quart_str_md)
                m_id_md.append(mean_id_md)
                dev_id_md.append(std_id_md)
                maximum_id_md.append(max_id_md)
                q3_id_md.append(quart_id_md)
                flat_id_md.append(null_id_md)
                flat_string_md.append(null_string_md)
                count_url_md.append(url_md)
                obf_id_md.append(obf_md)
                obf_string_md.append(obf_md_str)
        # assign columns to the existing dataframe
        pd.options.mode.chained_assignment = None
        import warnings
        warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)
        # note that this type of assignment is not efficient
        database['presence of installation script']=installation
        database['shannon mean ID source code']=m_id_sc
        database['shannon std ID source code']=dev_id_sc
        database['shannon max ID source code']=maximum_id_sc
        database['shannon q3 ID source code']=q3_id_sc
        database['shannon mean string source code']=m_str_sc
        database['shannon std string source code']=dev_str_sc
        database['shannon max string source code']=maximum_str_sc
        database['shannon q3 string source code']=q3_str_sc
        database['homogeneous identifiers in source code']=flat_id_sc
        database['homogeneous strings in source code']=flat_string_sc
        database['heteregeneous identifiers in source code']=obf_id_sc
        database['heterogeneous strings in source code']=obf_string_sc
        database['URLs in source code']=count_url_sc
        # metadata features
        database['shannon mean ID metadata']=m_id_md
        database['shannon std ID metadata']=dev_id_md
        database['shannon max ID metadata']=maximum_id_md
        database['shannon q3 ID metadata']=q3_id_md
        database['shannon mean string metadata']=m_str_md
        database['shannon std string metadata']=dev_str_md
        database['shannon max string metadata']=maximum_str_md
        database['shannon q3 string metadata']=q3_str_md
        database['homogeneous identifiers in metadata']=flat_id_md
        database['homogeneous strings in metadata']=flat_string_md
        database['heterogeneous strings in metadata']=obf_string_md
        database['URLs in metadata']=count_url_md
        database['heteregeneous identifiers in metadata']=obf_id_md
        # drop code_x and code_y: raw source code and metadata
        database.drop(['strings_x', 'strings_y','identifiers_x','identifiers_y'], axis=1, inplace=True) 
        # remove duplicates based on some numeric features 
        database.drop_duplicates(subset=['Number of words_x','Number of words_y','lines_x','lines_y','repository','presence of installation script'],keep='first',inplace=True)
        # change the column name of Number of Words_x, Number of Words_y in Number of Words in source code, Number of Words in metadata
        database.rename(columns={'Number of words_x':'Number of Words in source code'},inplace=True)
        database.rename(columns={'Number of words_y':'Number of Words in metadata'},inplace=True)
        database.rename(columns={'lines_x':'Number of lines in source code'},inplace=True)
        database.rename(columns={'lines_y':'Number of lines in metadata'},inplace=True)
        database.rename(columns={'IP_x':'Number of IP adress in source code'},inplace=True)
        database.rename(columns={'base64_x':'Number of base64 chunks in source code'},inplace=True)
        database.rename(columns={'sospicious token_x':'Number of sospicious token in source code'},inplace=True)
        database.rename(columns={'IP_y':'Number of IP adress in metadata'},inplace=True)
        database.rename(columns={'base64_y':'Number of base64 chunks in metadata'},inplace=True)
        database.rename(columns={'sospicious token_y':'Number of sospicious token in metadata'},inplace=True)
        return (database)
                
